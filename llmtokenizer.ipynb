{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM5iOCnh3SeFSIKyQSJZLVw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rudra-rudie/llmbuilding/blob/main/llmtokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6oDRrwiRQQuy",
        "outputId": "11c40287-cc2b-4604-cf67-63bd5de0f325"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of character: 138\n",
            "This is a sample text for tokenization. It's designed to demonstrate how the tokenizer works. The q\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "# Using a sample text directly for demonstration purposes\n",
        "# This replaces the previous attempt to read a PDF, which resulted in garbled text.\n",
        "raw_text = \"This is a sample text for tokenization. It's designed to demonstrate how the tokenizer works. The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "print (\"Total number of character:\", len(raw_text))\n",
        "print(raw_text[:99])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text = \"Hello, world. This, is a test.\"\n",
        "result = re.split(r'(\\s)', text)\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gf9eh0MmUSBl",
        "outputId": "ecd99cbb-f6d5-4137-e40a-f587ffcec04f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = re.split(r'([,.]|\\s)',text)\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rmh72Z_xXLz_",
        "outputId": "b7f5ec17-b6dc-4a9e-8d9e-4c34c0f460c3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = [item for item in result if item.strip()]\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1I9_f28XleA",
        "outputId": "9a2dd4f1-d0eb-41b3-d105-208d77b9f323"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello world. Is this-- a test?\"\n",
        "result = re.split(r'([,.:?_!\"()\\\"]|--|\\s)', text)\n",
        "result = [item.strip() for item in result if item.strip()]\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OESVK0fZX8iE",
        "outputId": "ee2a4816-f57f-4c52-9477-8728e9e1c9bd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = [item for item in result if item.strip()]\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VsULpJDfZDiO",
        "outputId": "f6aec059-7113-4807-fc3c-ae71e8c645a0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed = re.split(r\"([,.:;?_!\\\"()']|--|\\s)\", raw_text)\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "print(preprocessed[:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ju9bpRfzZaWB",
        "outputId": "ebef11b8-845b-49e2-8b41-ab592589c0d7"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'a', 'sample', 'text', 'for', 'tokenization', '.', 'It', \"'\", 's', 'designed', 'to', 'demonstrate', 'how', 'the', 'tokenizer', 'works', '.', 'The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(preprocessed))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RS8lwovaRrR",
        "outputId": "31945912-12ef-438b-d488-63ca7398612d"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = sorted(set(preprocessed ))\n",
        "vocab_size = len(all_words)\n",
        "\n",
        "print (vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CaJk8xdaYIt",
        "outputId": "8125443b-47a1-4cb6-a030-45a9d3802a51"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Fsb0649SdSGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {token: integer for integer,token in enumerate(all_words)}"
      ],
      "metadata": {
        "id": "z0TLJ-hrbHp6"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i , item in enumerate (vocab.items()):\n",
        "  print(item)\n",
        "  if i>= 50:\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRPCgxfjdiQ9",
        "outputId": "dfddc9da-415d-42ac-c9fd-0392846263c4"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(\"'\", 0)\n",
            "('.', 1)\n",
            "('It', 2)\n",
            "('The', 3)\n",
            "('This', 4)\n",
            "('a', 5)\n",
            "('brown', 6)\n",
            "('demonstrate', 7)\n",
            "('designed', 8)\n",
            "('dog', 9)\n",
            "('for', 10)\n",
            "('fox', 11)\n",
            "('how', 12)\n",
            "('is', 13)\n",
            "('jumps', 14)\n",
            "('lazy', 15)\n",
            "('over', 16)\n",
            "('quick', 17)\n",
            "('s', 18)\n",
            "('sample', 19)\n",
            "('text', 20)\n",
            "('the', 21)\n",
            "('to', 22)\n",
            "('tokenization', 23)\n",
            "('tokenizer', 24)\n",
            "('works', 25)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizer:\n",
        "  def __init__(self, vocab):\n",
        "    self.str_to_int = vocab\n",
        "    self.int_to_str = {i: s for s, i in vocab.items()}\n",
        "\n",
        "  def encode(self, text):\n",
        "    # Corrected regex to be consistent with the vocabulary generation\n",
        "    # Using double-quoted raw string for better handling of literal single quotes\n",
        "    preprocessed = re.split(r\"([,.:;?_!\\\"()']|--|\\s)\", text)\n",
        "\n",
        "    preprocessed = [\n",
        "        item.strip() for item in preprocessed if item.strip()\n",
        "    ]\n",
        "    ids = [self.str_to_int[s] for s in preprocessed ]\n",
        "    return ids\n",
        "\n",
        "  def decode(self, ids):\n",
        "    text = \" \".join([self.int_to_str[i]for i in ids])\n",
        "    # Adjusted regex for decode to match the encoding pattern for punctuation\n",
        "    # Corrected escaping of ')' inside character class\n",
        "    text =  re.sub(r\"\\s+([,.:;?_!\\\"()'])\", r\"\\1\", text)\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "899VS3FPd0Qz"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizer(vocab)\n",
        "\n",
        "# Using raw_text itself to ensure all tokens are in the vocabulary\n",
        "text = raw_text\n",
        "ids = tokenizer.encode(text)\n",
        "print(ids)\n",
        "\n",
        "decoded_text = tokenizer.decode(ids)\n",
        "print(f\"\\nEncoded IDs: {ids}\")\n",
        "print(f\"Decoded Text: {decoded_text}\")\n",
        "\n",
        "# Optional: Test with the original text from the problem statement (will still KeyError without expanding vocab)\n",
        "# text_from_problem =\"\"\" It's the last he painted you know,\"\n",
        "#            Mrs. Gisburn said with pardonable pride.\"\"\"\n",
        "# try:\n",
        "#     ids_problem = tokenizer.encode(text_from_problem)\n",
        "#     print(f\"\\nIDs from problem text: {ids_problem}\")\n",
        "# except KeyError as e:\n",
        "#     print(f\"\\nKeyError for original problem text: {e}. Tokens not in current vocabulary.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOPTk9xOgmG3",
        "outputId": "50abea78-5b17-415e-b455-1e4101eb8f90"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4, 13, 5, 19, 20, 10, 23, 1, 2, 0, 18, 8, 22, 7, 12, 21, 24, 25, 1, 3, 17, 6, 11, 14, 16, 21, 15, 9, 1]\n",
            "\n",
            "Encoded IDs: [4, 13, 5, 19, 20, 10, 23, 1, 2, 0, 18, 8, 22, 7, 12, 21, 24, 25, 1, 3, 17, 6, 11, 14, 16, 21, 15, 9, 1]\n",
            "Decoded Text: This is a sample text for tokenization. It' s designed to demonstrate how the tokenizer works. The quick brown fox jumps over the lazy dog.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "wrGl9G1-hOYX",
        "outputId": "1cd2d854-a616-4e86-b72f-2352e009a6f2"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"This is a sample text for tokenization. It' s designed to demonstrate how the tokenizer works. The quick brown fox jumps over the lazy dog.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_tokens = sorted(list(set(preprocessed)))\n",
        "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
        "\n",
        "vocab = {token: integer for integer,token in enumerate(all_tokens)}\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cRrzjjurj9Nb"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocab.items())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0qBlxpYlvfp",
        "outputId": "d8d67f4c-3f72-459c-854f-195280ec7199"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i , item in enumerate(list(vocab.items())[-5]):\n",
        "  print(item)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9ehOzTQly0t",
        "outputId": "de448aab-4de0-4111-b690-6c8271df2beb"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenization\n",
            "23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimmpleTokenizerV2:\n",
        "  def __init__(self, vocab):\n",
        "    self.str_to_int = vocab\n",
        "    self.int_to_str = {v: k for k, v in vocab.items()}\n",
        "\n",
        "  def encode(self, text):\n",
        "    preprocessed = re.split(r'([,.:;?_!\"()\\\\]|--|\\s)', text)\n",
        "    preprocessed = [\n",
        "                    item if item in self.str_to_int\n",
        "                    else\"<|unk|>\" for item in preprocessed\n",
        "    ]\n",
        "\n",
        "    ids = [self.str_to_int[item] for item in preprocessed]\n",
        "    return ids\n",
        "\n",
        "  def decode(self,ids):\n",
        "    text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "\n",
        "    text = re.sub(r'\\s+([,.:;?!\"()\\\\])',r'\\1',text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "GlE8L472mJ9m"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimmpleTokenizerV2(vocab)\n",
        "\n",
        "text1 = \"Hello , do u like tea?\"\n",
        "text2 = \"IN the sunlit terraces of the palace.\"\n",
        "\n",
        "text = \"<|endoftext|>\".join ((text1,text2))\n",
        "\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmrnxciFn2Pn",
        "outputId": "3c129893-aa40-46cd-d6a8-56a49c94a58e"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello , do u like tea?<|endoftext|>IN the sunlit terraces of the palace.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.encode(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2qtEjhsoVRu",
        "outputId": "2e5546f9-8daa-4f80-bb78-efd8c42c54ce"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[27,\n",
              " 27,\n",
              " 27,\n",
              " 27,\n",
              " 27,\n",
              " 27,\n",
              " 27,\n",
              " 27,\n",
              " 27,\n",
              " 27,\n",
              " 27,\n",
              " 27,\n",
              " 27,\n",
              " 27,\n",
              " 27,\n",
              " 27,\n",
              " 21,\n",
              " 27,\n",
              " 27,\n",
              " 27,\n",
              " 27,\n",
              " 27,\n",
              " 27,\n",
              " 27,\n",
              " 21,\n",
              " 27,\n",
              " 27,\n",
              " 1,\n",
              " 27]"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(tokenizer.encode(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "-pw_mmtzosBb",
        "outputId": "5f82f274-3995-4ffc-9ce8-86ccc78832d6"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<|unk|> <|unk|> <|unk|> <|unk|> <|unk|> <|unk|> <|unk|> <|unk|> <|unk|> <|unk|> <|unk|> <|unk|> <|unk|> <|unk|> <|unk|> <|unk|> the <|unk|> <|unk|> <|unk|> <|unk|> <|unk|> <|unk|> <|unk|> the <|unk|> <|unk|>. <|unk|>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rjV4Fth9ozCE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}